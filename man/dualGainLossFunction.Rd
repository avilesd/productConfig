% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DRPFunctions.R
\name{dualGainLossFunction}
\alias{dualGainLossFunction}
\title{Calculates the gain-loss matrix from a decision matrix}
\usage{
dualGainLossFunction(aMatrix, dual.refps)
}
\arguments{
\item{aMatrix}{the decision matrix, with \code{attr} as columns and
\code{rounds} as rows.}

\item{dual.refps}{two numeric reference points (status-quo, goal).}
}
\value{
a gain-loss matrix with equal dimensions as the input \code{aMatrix}
}
\description{
This function is called by \code{\link{smallerThanZero}} for its second step. It
takes a matrix and runs the gain-loss function over each value of the matrix. The gain-loss
function returns a positive value (gain) for values larger than the status-quo and negative values (loss)
for smaller than the \code{sq}.
}
\details{
The matematical function used here is the one given by [1] and since it is composed
of a logarithmic function it does not accept negative values.
}
\examples{
#Runnable
dualGainLossFunction(matrix(101:300, 20, 10, byrow= T), dual.refps= c(142, 195))
dualGainLossFunction(matrix(16:31, 4, 4), dual.refps= c(20, 25))

}
\references{
[1] Golman, R., & Loewenstein, G. (2011). Explaining Nonconvex
  Preferences with Aspirational and Status Quo Reference Dependence. Mimeo,
  Carnegie Mellon University.
}

